{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../gym_art')\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from paintings import PaintingEnv, load_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PaintingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalAgent(object):\n",
    "    def __init__(self, env):\n",
    "        # The memory of all the paintings and their values\n",
    "        self.env = env\n",
    "        self.paint_to_vals = { artist : None for artist in env.paintings }\n",
    "\n",
    "    def act(self, trial_info):\n",
    "        sides = ['left', 'right']\n",
    "        vals  = np.array([ self.paint_to_vals[trial_info[s]['name']] for s in sides ])\n",
    "        nulls = pd.isnull(vals)\n",
    "\n",
    "        # If both are None\n",
    "        if nulls.all():\n",
    "            action = random.choice([0,1])\n",
    "        # If one is None then choose that one\n",
    "        elif nulls.any():\n",
    "            action = np.argmax(nulls)\n",
    "        # If neither is none, then choose the higher valued one\n",
    "        else:\n",
    "            action = np.argmax(vals)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def step(self, trial_info, action, reward):\n",
    "        sides = ['left', 'right']\n",
    "        chosen_side = sides[action]\n",
    "        painting = trial_info[chosen_side]['name']\n",
    "        if self.paint_to_vals[painting] is None:\n",
    "            self.paint_to_vals[painting] = reward\n",
    "        elif self.paint_to_vals[painting] != reward:\n",
    "            raise Exception(\"Reward value has changed\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "for i in range(10):\n",
    "    state, reward, done, info = env.reset(full_reset=True) # reset environment\n",
    "    agent = OptimalAgent(env) # Call on the agent\n",
    "    score = 0\n",
    "    for t in range(env.n_trials - 1):\n",
    "        action = agent.act(info)                   # select an action\n",
    "        next_state, reward, done, next_info = env.step(action)   # send action to environment\n",
    "        agent.step(info, action, reward) # learning step\n",
    "        state = next_state\n",
    "        info = next_info\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    mean_score = score/t # keep scores in range of 0-1, allow comparison to humans above\n",
    "    mean_scores.append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5138636363636363"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the average optimal score\n",
    "np.array(mean_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
